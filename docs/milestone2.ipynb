{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Differentiation, or the process of finding a derivative, is an extremely important mathematical operation with a wide range of applications. The discovery of extrema or zeros of functions is essential in any optimization problem, and the solving of differential equations is fundamental to modern science and engineering. Differentiation is essential in nearly all quantitative disciplines: physicists may take the derivative of the displacement of a moving object with respect to time in order to find the velocity of that object, and data scientists may use derivatives when optimizing weights in a neural network. \n",
    "\n",
    "Naturally, we would like to compute it as accurately and efficiently as possible.\n",
    "Two classical methods of calculating the derivative have clear shortcomings. Symbolic differentiation (finding the derivative of a given formula with respect to a specified variable, producing a new formula as its output) will be accurate, but can be quite expensive computationally. The finite difference method ($\\frac{\\partial f}{\\partial x} = \\frac{f(x+\\epsilon)-f(x)}{\\epsilon}$ for some small $\\epsilon$) does not have this issue, but will be less precise as different values of epsilon will give different results. This brings us to automatic differentiation, a less costly and more precise approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Automatic differentiation breaks down the main function into elementary functions, evaluated upon one another. It then uses the chain rule to update the derivative at each step and ends in the derivative of the entire function.\n",
    "\n",
    "To better understand this process, let's look at an example. Consider the example function\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = x + 4(sin(\\frac{x}{4}))\n",
    "\\end{equation}\n",
    "\n",
    "We would like to compute the derivative of this function at a particular value of x. Let's say that in this case, we are interested in evaluating the derivative at $x=\\pi$. In other words, we want to find $f'(\\pi)$ where $f'(x) = \\frac{\\partial f}{\\partial x}$\n",
    "\n",
    "We know how to solve this _symbolically_ using methods that we learned in calculus, but remember, we want to compute this answer as accurately and efficiently as possible, which is why we want to solve it using automatic differentiation. \n",
    "\n",
    "To solve this using automatic differentiation, we need to find the decomposition of the differentials provied by the chain rule. Remember, the chain rule is a formula for computing the derivative of the composition of two or more functions. So if we have a function $h\\left(u\\left(t\\right)\\right)$ and we want the derivative of $h$ with respect to $t$, then we know by the chain rule that the derivative is $\\dfrac{\\partial h}{\\partial t} = \\dfrac{\\partial h}{\\partial u}\\dfrac{\\partial u}{\\partial t}.$ The chain rule can also be expanded to deal with multiple arguments and vector inputs (in which case we would be calculating the _gradient)_.\n",
    "\n",
    "Our function $f(x)$ is composed of elemental functions for which we know the derivatives. We will separate out each of these elemental functions, evaluating the derivative at each step using the chain rule. \n",
    "\n",
    "The evaluation trace for this problem looks like:\n",
    "\n",
    "\n",
    "| Trace    | Elementary Operation &nbsp;&nbsp;&nbsp;| Derivative &nbsp;&nbsp;&nbsp; | $\\left(f\\left(a\\right),  f^{\\prime}\\left(a\\right)\\right)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n",
    "| :------: | :----------------------:               | :------------------------------: | :------------------------------: |\n",
    "| $x_{1}$  | $\\pi$                      | $1$                | $\\left(\\pi, 1\\right)$ |\n",
    "| $x_{2}$  | $\\frac{x_{1}}{4}$                               | $\\frac{\\dot{x}_{1}}{4}$                 | $\\left(\\dfrac{\\pi}{4}, \\frac{1}{4}\\right)$ |\n",
    "| $x_{3}$  | $\\sin\\left(x_{2}\\right)$               | $\\cos\\left(x_{2}\\right)\\dot{x}_{2}$            | $\\left(\\dfrac{\\sqrt{2}}{2}, 2\\sqrt{2}\\right)$ |\n",
    "| $x_{4}$  | $4x_{3}$                               | $4\\dot{x}_{3}$                  | $\\left(2\\sqrt{2}, 8\\sqrt{2}\\right)$ |\n",
    "| $x_{5}$  | $x_{1} + x_{4}$                        | $\\dot{x}_{1} + \\dot{x}_{4}$ | $\\left(\\pi + 2\\sqrt{2}, 1 + 8\\sqrt{2}\\right)$ |\n",
    "\n",
    "\n",
    "This evaluation trace provides some intuition for how automatic differentiation (in the forward mode) is used to calculate the derivative of a function evaluated at a certain value ($f'(\\pi) = 1 + 8\\sqrt{2}$). \n",
    "\n",
    "You may notice that when we computed the derivative above, we \"seeded\" the derivative with a value of 1. This seed vector doesn't have to be 1, but the utility of using a unit vector becomes apparent when we consider a problem involving directional derivatives.\n",
    "\n",
    "The definition of the directional derivative (where p is the seed vector)\n",
    "$$D_{p}x_{3} = \\sum_{j=1}^{2}{\\dfrac{\\partial x_{3}}{\\partial x_{j}}p_{j}}$$\n",
    "\n",
    "can be expanded to\n",
    "\n",
    "\\begin{align}\n",
    "  D_{p}x_{3} &= \\dfrac{\\partial x_{3}}{\\partial x_{1}}p_{1} + \\dfrac{\\partial x_{3}}{\\partial x_{2}}p_{2} \\\\\n",
    "             &= x_{2}p_{1} + x_{1}p_{2}\n",
    "\\end{align}\n",
    "\n",
    "If we choose p to be a the unit vector, we can see how this is beneficial:\n",
    "\n",
    "$p = \\left(1,0\\right)$ gives $\\dfrac{\\partial f}{\\partial x}$\n",
    "\n",
    "$p = \\left(0,1\\right)$ gives $\\dfrac{\\partial f}{\\partial y}$\n",
    "\n",
    "So to summarize, the forward mode of automatic differentiation is really computing the _**product of the gradient of our function with the seed vector:**_\n",
    "\n",
    "$$D_{p}x_{3} = \\nabla x_{3}\\cdot p.$$ \n",
    "\n",
    "If our function is a vector, then the forward mode actually computes $Jp$ where $J = \\dfrac{\\partial f_{i}}{\\partial x_{j}}, \\quad i = 1,\\ldots, n, \\quad j = 1,\\ldots,m$ is the Jacobian matrix. Often we will really only want the \"action\" of the Jacobian on a vector, so we will just want to compute the matrix-vector product $Jp$ for some vector $p$.\n",
    "\n",
    "Automatic differentiation can be used to compute derivatives to machine precision of functions $f:\\mathbb{R}^{m} \\to \\mathbb{R}^{n}$\n",
    "\n",
    "Forward mode is more efficient when $n\\gg m$.\n",
    "   - This corresponds to the case where the number of functions to evaluate is much greater than the number of inputs.\n",
    "   - Actually computes the Jacobian-vector product $Jp$.\n",
    "   \n",
    "Reverse mode is more efficient when $n\\ll m$.\n",
    "   - This corresponds to the case where the number of inputs is much greater than the number of functions.\n",
    "   - Actually computes the Jacobian-transpose-vector product $J^{T}p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use autodiff32\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Create a virtual environment (optional)**\n",
    "\n",
    "From the terminal, create a virtual environment:\n",
    "\n",
    "_(The command below will create the virtual environment in your present working directory, so consider moving to a project folder or a known location before creating the environment)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```virtualenv env```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activate the virtual environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```source env/bin/activate```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you plan to launch a jupyter notebook using this virtual environment, run the following to install and set up jupyter in your virtual environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python -m pip install jupyter```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python -m ipykernel install --user --name=env```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Install the autodiff32 package**\n",
    "\n",
    "In the terminal, type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pip install autodiff32```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package dependencies will be taken care of automatically!\n",
    "\n",
    "_(Alternatively, it is also possible to install the autodiff32 package by downloading this GitHub repository. If you choose that method, use the requirements.txt file to ensure you have installed all necessary dependencies.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use\n",
    "\n",
    "It is easy to use the autodiff32 package in a Jupyter notebook, as we will demonstrate here:\n",
    "\n",
    "_(Alternatively, you can start a Python interpreter by typing ```Python``` into the terminal, or work from your favorite Python IDE.)_\n",
    "\n",
    "_(Remember, if you are using a virtual environment, follow steps 1 through 3 above and then type ```jupyter notebook``` into your terminal to launch a notebook. Inside the notebook, switch the kernel to that of your virtual environment.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derivative: 32.187521164486526\n",
      "value: 2.426486643551989\n"
     ]
    }
   ],
   "source": [
    "import autodiff32 as ad\n",
    "\n",
    "\"\"\"\n",
    "Initialize an AutoDiff object  \n",
    "with the number you would like to pass into your function\n",
    "\"\"\"\n",
    "X = ad.AutoDiff(3)\n",
    "\n",
    "\"\"\"\n",
    "Define your function of interest\n",
    "\"\"\"\n",
    "func = 1/ad.sin(X**2)\n",
    "\n",
    "\"\"\"\n",
    "Look at the derivative of your function \n",
    "evaluated at the number you gave above\n",
    "\"\"\"\n",
    "print(\"derivative:\",func.der)\n",
    "\n",
    "\"\"\"\n",
    "Look at the value of your function \n",
    "evaluated at the number you gave:\n",
    "\"\"\"\n",
    "print(\"value:\", func.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "### Directory Structure\n",
    "Our structure is as follows:\n",
    "\n",
    "    /cs207-FinalProject\n",
    "        README.md\n",
    "        LICENSE\n",
    "        .gitignore\n",
    "        .travis.yml\n",
    "        setup.py\n",
    "        requirements.txt\n",
    "        docs/\n",
    "            milestone1.ipynb\n",
    "            milestone2.ipynb\n",
    "        autodif32/\n",
    "            __init__.py\n",
    "            AutoDiffObj.py\n",
    "            Elementary.py\n",
    "            JacobianVectorFunc.py\n",
    "            MultivariateVarCreator.py\n",
    "        test/\n",
    "            __init__.py\n",
    "            autodiffobj_test.py\n",
    "            elementary_test.py\n",
    "\n",
    "### Basic Modules\n",
    "< Insert information about basic modules and what they do>\n",
    "\n",
    "### Testing Suite\n",
    "Our testing files live in the `test/` directory. They run using the pytest.\n",
    "\n",
    "## Implementation\n",
    "< Insert information about implementation >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
