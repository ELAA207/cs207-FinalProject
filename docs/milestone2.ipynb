{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "milestone2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSnGVAi15-59",
        "colab_type": "text"
      },
      "source": [
        "## Milestone 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbuQK0q65-6F",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Differentiation, or the process of finding a derivative, is an extremely important mathematical operation with a wide range of applications. The discovery of extrema or zeros of functions is essential in any optimization problem, and the solving of differential equations is fundamental to modern science and engineering. Differentiation is essential in nearly all quantitative disciplines: physicists may take the derivative of the displacement of a moving object with respect to time in order to find the velocity of that object, and data scientists may use derivatives when optimizing weights in a neural network. \n",
        "\n",
        "Naturally, we would like to compute it as accurately and efficiently as possible.\n",
        "Two classical methods of calculating the derivative have clear shortcomings. Symbolic differentiation (finding the derivative of a given formula with respect to a specified variable, producing a new formula as its output) will be accurate, but can be quite expensive computationally. The finite difference method ($\\frac{\\partial f}{\\partial x} = \\frac{f(x+\\epsilon)-f(x)}{\\epsilon}$ for some small $\\epsilon$) does not have this issue, but will be less precise as different values of epsilon will give different results. This brings us to automatic differentiation, a less costly and more precise approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f63jAUyI5-6I",
        "colab_type": "text"
      },
      "source": [
        "## Background\n",
        "\n",
        "Automatic differentiation breaks down the main function into elementary functions, evaluated upon one another. It then uses the chain rule to update the derivative at each step and ends in the derivative of the entire function.\n",
        "\n",
        "To better understand this process, let's look at an example. Consider the example function\n",
        "\n",
        "\\begin{equation}\n",
        "f(x) = x + 4(sin(\\frac{x}{4}))\n",
        "\\end{equation}\n",
        "\n",
        "We would like to compute the derivative of this function at a particular value of x. Let's say that in this case, we are interested in evaluating the derivative at $x=\\pi$. In other words, we want to find $f'(\\pi)$ where $f'(x) = \\frac{\\partial f}{\\partial x}$\n",
        "\n",
        "We know how to solve this _symbolically_ using methods that we learned in calculus, but remember, we want to compute this answer as accurately and efficiently as possible, which is why we want to solve it using automatic differentiation. \n",
        "\n",
        "To solve this using automatic differentiation, we need to find the decomposition of the differentials provied by the chain rule. Remember, the chain rule is a formula for computing the derivative of the composition of two or more functions. So if we have a function $h\\left(u\\left(t\\right)\\right)$ and we want the derivative of $h$ with respect to $t$, then we know by the chain rule that the derivative is $\\dfrac{\\partial h}{\\partial t} = \\dfrac{\\partial h}{\\partial u}\\dfrac{\\partial u}{\\partial t}.$ The chain rule can also be expanded to deal with multiple arguments and vector inputs (in which case we would be calculating the _gradient)_.\n",
        "\n",
        "Our function $f(x)$ is composed of elemental functions for which we know the derivatives. We will separate out each of these elemental functions, evaluating the derivative at each step using the chain rule. \n",
        "\n",
        "The evaluation trace for this problem looks like:\n",
        "\n",
        "\n",
        "| Trace    | Elementary Operation &nbsp;&nbsp;&nbsp;| Derivative &nbsp;&nbsp;&nbsp; | $\\left(f\\left(a\\right),  f^{\\prime}\\left(a\\right)\\right)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n",
        "| :------: | :----------------------:               | :------------------------------: | :------------------------------: |\n",
        "| $x_{1}$  | $\\pi$                      | $1$                | $\\left(\\pi, 1\\right)$ |\n",
        "| $x_{2}$  | $\\frac{x_{1}}{4}$                               | $\\frac{\\dot{x}_{1}}{4}$                 | $\\left(\\dfrac{\\pi}{4}, \\frac{1}{4}\\right)$ |\n",
        "| $x_{3}$  | $\\sin\\left(x_{2}\\right)$               | $\\cos\\left(x_{2}\\right)\\dot{x}_{2}$            | $\\left(\\dfrac{\\sqrt{2}}{2}, 2\\sqrt{2}\\right)$ |\n",
        "| $x_{4}$  | $4x_{3}$                               | $4\\dot{x}_{3}$                  | $\\left(2\\sqrt{2}, 8\\sqrt{2}\\right)$ |\n",
        "| $x_{5}$  | $x_{1} + x_{4}$                        | $\\dot{x}_{1} + \\dot{x}_{4}$ | $\\left(\\pi + 2\\sqrt{2}, 1 + 8\\sqrt{2}\\right)$ |\n",
        "\n",
        "\n",
        "This evaluation trace provides some intuition for how automatic differentiation (in the forward mode) is used to calculate the derivative of a function evaluated at a certain value ($f'(\\pi) = 1 + 8\\sqrt{2}$). \n",
        "\n",
        "You may notice that when we computed the derivative above, we \"seeded\" the derivative with a value of 1. This seed vector doesn't have to be 1, but the utility of using a unit vector becomes apparent when we consider a problem involving directional derivatives.\n",
        "\n",
        "The definition of the directional derivative (where p is the seed vector)\n",
        "$$D_{p}x_{3} = \\sum_{j=1}^{2}{\\dfrac{\\partial x_{3}}{\\partial x_{j}}p_{j}}$$\n",
        "\n",
        "can be expanded to\n",
        "\n",
        "\\begin{align}\n",
        "  D_{p}x_{3} &= \\dfrac{\\partial x_{3}}{\\partial x_{1}}p_{1} + \\dfrac{\\partial x_{3}}{\\partial x_{2}}p_{2} \\\\\n",
        "             &= x_{2}p_{1} + x_{1}p_{2}\n",
        "\\end{align}\n",
        "\n",
        "If we choose p to be a the unit vector, we can see how this is beneficial:\n",
        "\n",
        "$p = \\left(1,0\\right)$ gives $\\dfrac{\\partial f}{\\partial x}$\n",
        "\n",
        "$p = \\left(0,1\\right)$ gives $\\dfrac{\\partial f}{\\partial y}$\n",
        "\n",
        "So to summarize, the forward mode of automatic differentiation is really computing the _**product of the gradient of our function with the seed vector:**_\n",
        "\n",
        "$$D_{p}x_{3} = \\nabla x_{3}\\cdot p.$$ \n",
        "\n",
        "If our function is a vector, then the forward mode actually computes $Jp$ where $J = \\dfrac{\\partial f_{i}}{\\partial x_{j}}, \\quad i = 1,\\ldots, n, \\quad j = 1,\\ldots,m$ is the Jacobian matrix. Often we will really only want the \"action\" of the Jacobian on a vector, so we will just want to compute the matrix-vector product $Jp$ for some vector $p$.\n",
        "\n",
        "Automatic differentiation can be used to compute derivatives to machine precision of functions $f:\\mathbb{R}^{m} \\to \\mathbb{R}^{n}$\n",
        "\n",
        "Forward mode is more efficient when $n\\gg m$.\n",
        "   - This corresponds to the case where the number of functions to evaluate is much greater than the number of inputs.\n",
        "   - Actually computes the Jacobian-vector product $Jp$.\n",
        "   \n",
        "Reverse mode is more efficient when $n\\ll m$.\n",
        "   - This corresponds to the case where the number of inputs is much greater than the number of functions.\n",
        "   - Actually computes the Jacobian-transpose-vector product $J^{T}p$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOP5LRQS5-6K",
        "colab_type": "text"
      },
      "source": [
        "## How to Use autodiff32\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFgAgEKH5-6N",
        "colab_type": "text"
      },
      "source": [
        "**1) Create a virtual environment (optional)**\n",
        "\n",
        "From the terminal, create a virtual environment:\n",
        "\n",
        "_(The command below will create the virtual environment in your present working directory, so consider moving to a project folder or a known location before creating the environment)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jLJC1415-6P",
        "colab_type": "text"
      },
      "source": [
        "```virtualenv env```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYEESm3K5-6S",
        "colab_type": "text"
      },
      "source": [
        "activate the virtual environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWIfIblI5-6V",
        "colab_type": "text"
      },
      "source": [
        "```source env/bin/activate```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oaXDXJp5-6Z",
        "colab_type": "text"
      },
      "source": [
        "if you plan to launch a jupyter notebook using this virtual environment, run the following to install and set up jupyter in your virtual environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9pvXnnk5-6d",
        "colab_type": "text"
      },
      "source": [
        "```python -m pip install jupyter```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dANz0waE5-6f",
        "colab_type": "text"
      },
      "source": [
        "```python -m ipykernel install --user --name=env```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLdSog4e5-6h",
        "colab_type": "text"
      },
      "source": [
        "**3) Install the autodiff32 package**\n",
        "\n",
        "In the terminal, type:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "favZ1Dgc5-6i",
        "colab_type": "text"
      },
      "source": [
        "```pip install autodiff32```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC7j0AUK5-6k",
        "colab_type": "text"
      },
      "source": [
        "Package dependencies will be taken care of automatically!\n",
        "\n",
        "_(Alternatively, it is also possible to install the autodiff32 package by downloading this GitHub repository. If you choose that method, use the requirements.txt file to ensure you have installed all necessary dependencies.)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coq9S6-25-6n",
        "colab_type": "text"
      },
      "source": [
        "### How to use\n",
        "\n",
        "It is easy to use the autodiff32 package in a Jupyter notebook, as we will demonstrate here:\n",
        "\n",
        "_(Alternatively, you can start a Python interpreter by typing ```Python``` into the terminal, or work from your favorite Python IDE.)_\n",
        "\n",
        "_(Remember, if you are using a virtual environment, follow steps 1 through 3 above and then type ```jupyter notebook``` into your terminal to launch a notebook. Inside the notebook, switch the kernel to that of your virtual environment.)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDtitZkZ5-6o",
        "colab_type": "code",
        "outputId": "93927a56-9443-4c33-d292-92104598a97e",
        "colab": {}
      },
      "source": [
        "import autodiff32 as ad\n",
        "\n",
        "\"\"\"\n",
        "Initialize an AutoDiff object  \n",
        "with the number you would like to pass into your function\n",
        "\"\"\"\n",
        "X = ad.AutoDiff(3)\n",
        "\n",
        "\"\"\"\n",
        "Define your function of interest\n",
        "\"\"\"\n",
        "func = 1/ad.sin(X**2)\n",
        "\n",
        "\"\"\"\n",
        "Look at the derivative of your function \n",
        "evaluated at the number you gave above\n",
        "\"\"\"\n",
        "print(\"derivative:\",func.der)\n",
        "\n",
        "\"\"\"\n",
        "Look at the value of your function \n",
        "evaluated at the number you gave:\n",
        "\"\"\"\n",
        "print(\"value:\", func.val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "derivative: 32.187521164486526\n",
            "value: 2.426486643551989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtZYybyM5-6v",
        "colab_type": "text"
      },
      "source": [
        "## Software Organization\n",
        "### Directory Structure\n",
        "Our structure is as follows:\n",
        "\n",
        "    /cs207-FinalProject\n",
        "        README.md\n",
        "        LICENSE\n",
        "        .gitignore\n",
        "        .travis.yml\n",
        "        setup.py\n",
        "        requirements.txt\n",
        "        docs/\n",
        "            milestone1.ipynb\n",
        "            milestone2.ipynb\n",
        "        autodif32/\n",
        "            __init__.py\n",
        "            AutoDiffObj.py\n",
        "            Elementary.py\n",
        "            JacobianVectorFunc.py\n",
        "            MultivariateVarCreator.py\n",
        "        test/\n",
        "            __init__.py\n",
        "            autodiffobj_test.py\n",
        "            elementary_test.py\n",
        "            JacobianVectorFunc_test.py\n",
        "\n",
        "### Basic Modules\n",
        "numpy arrays will be basic module used in our implementation\n",
        "\n",
        "### Testing Suite\n",
        "Our testing files live in the `test/` directory. They run using the pytest.\n",
        "\n",
        "## Implementation\n",
        "The core data structure (and also external dependencies) for our implementation will be numpy arrays and the core classes are implemented below:\n",
        "\n",
        "\n",
        "1) An ***AutoDiff class*** which stores the current value and derivative for the current node. The class method will conatin overloading operators such as plus, minus, multiply etc.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Initialize an Automatic Differentiation object which stores its current value and derivative\n",
        "\n",
        "Note that the derivative needs to not be a scalar value\n",
        "\n",
        "For multivariable differentiation problem of a scalar function, the derivative will be a vector\n",
        "\"\"\"\n",
        "\n",
        "def __init__(self,value,der=1) :\n",
        "  self.val = val\n",
        "  self.der = der\n",
        "  \n",
        "def check_dimension(self):\n",
        "  \"\"\"\n",
        "  This method will be integrated in other class methods as well to check the dimensions when performing vector computations and catch appropriate errors\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  This method has not been implemented yet\n",
        "  \"\"\"\n",
        "\n",
        "#overloading operators to enable basic operations between classes and numbers (not exhaustive):\n",
        "\n",
        "\"\"\"\n",
        "These methods will differentiate cases in which other is a scalar, a vector, a class, or any child class\n",
        "\n",
        "All of the these operator will return AutoDiff classes\n",
        "\"\"\"\n",
        "def __mult__(self,other):\n",
        "def __rmult__(self,other):\n",
        "def __radd__(self,other): \n",
        "def __add__(self,other):\n",
        "  \n",
        "# compute Jacobian for the multivariate function\n",
        "def Jacobian(self):\n",
        "  return self.der\n",
        "\n",
        "\n",
        "#For univairate functions\n",
        "X = AutoDiff(3)\n",
        "func = 3*X + 2\n",
        "print(func.val)\n",
        "print（func.der)\n",
        "\n",
        "```\n",
        "\n",
        "Multivariate functions (scalar/vector both) can be also evaluated using the AutoDiff class as below, and the Jacobian of it will be a vector array:\n",
        "\n",
        "```python\n",
        "X = AutoDiff(1,np.arrray([1,0]))\n",
        "Y =  AutoDiff(1,np.array([0,1]))\n",
        "\n",
        "func = X + 2*Y\n",
        "```\n",
        "\n",
        "One thing should be noticed that while such way of defining and evaluating multivariate functions are feasible, it is very inconvenient for the users to keep track of the dimensionality of the derivatives. For,example, the above func definition will raise an error if Y's derivative is defined as np.array([0,0,1]). This potential problem in dimensionality will also be a pain in the neck in the error handling process in the development of code. \n",
        "\n",
        "\n",
        "The way we tackle this problem is to create a helper class called **Multi_AutoDiff_Creator** as descirbed below:\n",
        "\n",
        "2) A ***Multi_AutoDiff_Creator class*** which helps the user create variable objects from a multivariable function\n",
        "\n",
        "```python\n",
        "\n",
        "\"\"\"\n",
        "This class helps user initialize different variables from a multivariable function without explicitly specifying them respectively using AutoDiff class\n",
        "\n",
        "It will need to import AutoDiff Class\n",
        "\"\"\"\n",
        "\tdef __init__(self,*args,**kwargs):\n",
        "\t\tderi = np.identity(len(kwargs))\n",
        "\t\ti = 0 \n",
        "\t\tself.Vars =[]\n",
        "\t\tfor key, value in kwargs.items():\n",
        "\t\t\tself.List.append(AutoDiff(value,deri[i]))\n",
        "\t\t\ti+=1\n",
        "\n",
        "#Demo and comparison\n",
        "\n",
        "'''initiate a multivariable function using Multi_AutoDiff_Creator class'''\n",
        "X,Y= Multi_AutoDiff_Creator(X = 1., Y=3.).Vars\n",
        "func = X + 2*Y*X \n",
        "```\n",
        "\n",
        "\n",
        "Notice that this class only serves as a helper class to ensure that every variable created has the correct format of dimensionality. The class itself has no influence on the forward differentation process.\n",
        "\n",
        "For better calculation of the derivatives of our elementary functions, we also introduce our elementary function class\n",
        "\n",
        "3)An ***Elementary function*** file which calculate derivatives for elementary functions as described previously\n",
        "```python  \n",
        "    #Elementary Function Derivative (not exhaustive):\n",
        "    def exp(self,ADobj):\n",
        "    def tan(self,ADobj):\n",
        "    def sin(self,ADobj):\n",
        "    def cos(self.ADobj):\n",
        "# ...\n",
        "\n",
        "```\n",
        "\n",
        "4) A ***Jacobian*** class which helps the user compute the Jacobian matrix for vector functions\n",
        "```python \n",
        "class Jacobian:\n",
        "\tdef __init__(self,vector_func):\n",
        "\t\tself.function = vector_func\n",
        "\t\tself.l = len(self.function)\n",
        "\n",
        "\tdef value(self):\n",
        "\t\tJacob=[]\n",
        "\t\tfor i in range(self.l):\n",
        "\t\t\tJacob.append(self.function[i].der)\n",
        "\t\treturn np.array(Jacob)\n",
        "\n",
        "\n",
        "    x, y = ad.Multi_AutoDiff_Creator(x=2, y=3).Vars\n",
        "\n",
        "    #Deifne a vector function\n",
        "    func = np.array([x+y, 2*x*y])\n",
        "    Jacob = ad.Jacobian(func)\n",
        "    print(Jacob.value()) # this class method output the full jacobian matrix as an np array\n",
        "```\n",
        "\n",
        "5) An ***Advanced Feature Class*** which implement more advanced derivatives (such as the Hessian/ Backward propagation) which inherits the AD\n",
        "\n",
        "6) A ***Test*** class which tests existing module functionalities, which we have in our project already.\n",
        "\n",
        "#Aspects yet to be implemented\n",
        "1) A dimensionality check method in both the AutoDiff and the Jacobian class to help check the dimensionality of users' operations\n",
        "\n",
        "2) Error checking for all the trig functions such as sec, tanh, cosh, etc.\n",
        "\n",
        "\n",
        "3)The extra feature of back propagation\n",
        "\n",
        "4) Other Error Handling issues that might occur\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lme3N8vuCDi9",
        "colab_type": "text"
      },
      "source": [
        "## Future Features\n",
        "\n",
        "We would like to try to implement the reverse mode automatic differentiation algorithm for our future features. The directory structure and basic modules used will remain the same.Additional data structure might invlove trees or linked list if necessary for our implementation of the reverse mode automatic differentation.\n",
        "\n",
        "Since for reverse mode automatic differentiation will require the storage of all relatived derivatives for the final derivative propagation(we will need to keep intermediate data in the memory during the\n",
        "forward pass for used in the backpropagation), the primary challenge for implementing this new feature is to understand how to store all the derivatives (in particular what data structure should we use to store them) with respect to their related values and how to recursivley calculate the final gradient.\n",
        "\n",
        "The implementation can be a tree like structure in which when we are overloading operators, we find a way to store the related values and derivatives as children for the previous nodes and when all the nodes (in this case autodiff objects) are traversed, we use recursion to back propagate the \"tree\".\n",
        "\n",
        "Optimally, we would like to implement a very simple neural network finally using gradient descent (or back propagation algorithm) using our self implemented reverse mode with proper layers,nodes, loss and activation function etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK-HTV0W5-6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}