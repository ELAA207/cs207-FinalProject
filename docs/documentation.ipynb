{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autodiff32 Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Differentiation, or the process of finding a derivative, is an extremely important mathematical operation with a wide range of applications. The discovery of extrema or zeros of functions is essential in any optimization problem, and the solving of differential equations is fundamental to modern science and engineering. Differentiation is essential in nearly all quantitative disciplines: physicists may take the derivative of the displacement of a moving object with respect to time in order to find the velocity of that object, and data scientists may use derivatives when optimizing weights in a neural network. \n",
    "\n",
    "Naturally, we would like to compute the derivative as accurately and efficiently as possible. Two classical methods of calculating the derivative have clear shortcomings. Symbolic differentiation (finding the derivative of a given formula with respect to a specified variable, producing a new formula as its output) will be accurate, but can be quite expensive computationally. The finite difference method ($\\frac{\\partial f}{\\partial x} = \\frac{f(x+\\epsilon)-f(x)}{\\epsilon}$ for some small $\\epsilon$) does not have this issue, but will be less precise as different values of epsilon will give different results. This brings us to automatic differentiation, a less costly and more precise approach.\n",
    "\n",
    "__Extension:__ For this project, we have implemented forward-mode automatic differentiation as well as reverse-mode automatic differentiation. It is important to have both methods accessible to have accuracy as well as optimal efficiency. \n",
    "\n",
    "Automatic differentiation can be used to compute derivatives to machine precision of functions $f:\\mathbb{R}^{m} \\to \\mathbb{R}^{n}$\n",
    "\n",
    "The forward mode is more efficient when $n\\gg m$.\n",
    "   - This corresponds to the case where the number of functions to evaluate is much greater than the number of inputs.\n",
    "   - Actually computes the Jacobian-vector product $Jp$.\n",
    "   \n",
    "The reverse mode is more efficient when $n\\ll m$.\n",
    "   - This corresponds to the case where the number of inputs is much greater than the number of functions.\n",
    "   - Actually computes the Jacobian-transpose-vector product $J^{T}p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Automatic differentiation breaks down the main function into elementary functions, evaluated upon one another. It then uses the chain rule to update the derivative at each step and ends in the derivative of the entire function.\n",
    "\n",
    "To better understand this process, let's look at an example. Consider the example function\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = x + 4sin(\\frac{x}{4})\n",
    "\\end{equation}\n",
    "\n",
    "We would like to compute the derivative of this function at a particular value of x. Let's say that in this case, we are interested in evaluating the derivative at $x=\\pi$. In other words, we want to find $f'(\\pi)$ where $f'(x) = \\frac{\\partial f}{\\partial x}$\n",
    "\n",
    "We know how to solve this _symbolically_ using methods that we learned in calculus, but remember, we want to compute this answer as accurately and efficiently as possible, which is why we want to solve it using automatic differentiation. \n",
    "\n",
    "### The Chain Rule\n",
    "\n",
    "To solve this using automatic differentiation, we need to find the decomposition of the differentials provied by the chain rule. Remember, the chain rule is a formula for computing the derivative of the composition of two or more functions. So if we have a function $h\\left(u\\left(t\\right)\\right)$ and we want the derivative of $h$ with respect to $t$, then we know by the chain rule that the derivative is $\\dfrac{\\partial h}{\\partial t} = \\dfrac{\\partial h}{\\partial u}\\dfrac{\\partial u}{\\partial t}.$ The chain rule can also be expanded to deal with multiple arguments and vector inputs (in which case we would be calculating the _gradient)_.\n",
    "\n",
    "Our function $f(x)$ is composed of elemental functions for which we know the derivatives. We will separate out each of these elemental functions, evaluating the derivative at each step using the chain rule. \n",
    "\n",
    "### Forward-mode differentiation\n",
    "\n",
    "Using forward-mode differentiation, the evaluation trace for this problem looks like:\n",
    "\n",
    "| Trace    | Elementary Operation | Derivative | $f'(a)$ |\n",
    "| :------: | :----------------------: | :------------------------------:  | :------------------------------: |\n",
    "| $x_{3}$  | $\\pi$ | $1$ | $\\pi$ | $1$ |\n",
    "| $x_{0}$  | $\\frac{x_{3}}{4}$ | $\\frac{\\dot{x}_{3}}{4}$ | $\\frac{1}{4}$ |\n",
    "| $x_{1}$  | $\\sin\\left(x_{0}\\right)$ | $\\cos\\left(x_{0}\\right)\\dot{x}_{0}$ | $\\frac{\\sqrt{2}}{8}$|\n",
    "| $x_{2}$  | $4x_{1}$ | $4\\dot{1}_{3}$ | $\\frac{\\sqrt{2}}{2}$ |\n",
    "| $x_{4}$  | $x_{2} + x_{3}$| $\\dot{x}_{2} + \\dot{x}_{3}$ | $1 + \\frac{\\sqrt{2}}{2}$ |\n",
    "\n",
    "This evaluation trace provides some intuition for how forward-mode automatic differentiation is used to calculate the derivative of a function evaluated at a certain value ($f'(\\pi) = 1 + \\frac{\\sqrt{2}}{2}$). \n",
    "\n",
    "<img src=\"computational_graph.png\"></img>\n",
    "*Figure 1: Forward-mode computational graph for example above*\n",
    "\n",
    "### Reverse-mode automatic differentiation\n",
    "\n",
    "Using reverse-mode differentiation, sometimes called backpropagation, the trace on the right is evaluated top to bottom intsead. \n",
    "\n",
    "| Trace    | Elementary Operation | Derivative &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| $f'(a)$ |\n",
    "| :------: | :----------------------: | :------------------------------: | :------------------------------: |\n",
    "| $x_{3}$  | $\\pi$ | $\\frac{\\partial{x_0}}{\\partial{x_3}}\\bar{x_0} + \\bar{x_4} = \\cos(\\frac{\\pi}{4}) + 1$ | $1 + \\frac{\\sqrt{2}}{2}$ |\n",
    "| $x_{0}$  | $\\frac{x_{3}}{4}$ | $\\frac{\\partial{x_1}}{\\partial{x_0}}\\bar{x_1} = 4\\cos(\\frac{\\pi}{4})$ | $2\\sqrt{2}$ |\n",
    "| $x_{1}$  | $\\sin\\left(x_{0}\\right)$ | $\\frac{\\partial{x_2}}{\\partial{x_1}}\\bar{x_2}$| $4$|\n",
    "| $x_{2}$  | $4x_{1}$ | $\\frac{\\partial{x_4}}{\\partial{x_2}}\\bar{x_4}$ | $1$ |\n",
    "| $x_{4}$  | $x_{2} + x_{3}$| $1$ | $1$ | $1$ \n",
    "\n",
    "<img src=\"reverse_graph.png\"></img>\n",
    "*Figure 2: Reverse-mode computational graph for example above*\n",
    "\n",
    "You may notice that when we computed the derivative above, we \"seeded\" the derivative with a value of 1. This seed vector doesn't have to be 1, but the utility of using a unit vector becomes apparent when we consider a problem involving directional derivatives.\n",
    "\n",
    "The definition of the directional derivative (where $p$ is the seed vector)\n",
    "$$D_{p}x_{3} = \\sum_{j=1}^{2}{\\dfrac{\\partial x_{3}}{\\partial x_{j}}p_{j}}$$\n",
    "\n",
    "can be expanded to\n",
    "\n",
    "\\begin{align}\n",
    "  D_{p}x_{3} &= \\dfrac{\\partial x_{3}}{\\partial x_{1}}p_{1} + \\dfrac{\\partial x_{3}}{\\partial x_{2}}p_{2} \\\\\n",
    "             &= x_{2}p_{1} + x_{1}p_{2}\n",
    "\\end{align}\n",
    "\n",
    "If we choose $p$ to be a the unit vector, we can see how this is beneficial:\n",
    "\n",
    "$p = \\left(1,0\\right)$ gives $\\dfrac{\\partial f}{\\partial x}$\n",
    "\n",
    "$p = \\left(0,1\\right)$ gives $\\dfrac{\\partial f}{\\partial y}$\n",
    "\n",
    "So to summarize, the forward mode of automatic differentiation is really computing the _product of the gradient of our function with the seed vector:_\n",
    "\n",
    "$$D_{p}x_{3} = \\nabla x_{3}\\cdot p.$$ \n",
    "\n",
    "If our function is a vector, then the forward mode actually computes $Jp$ where $J = \\dfrac{\\partial f_{i}}{\\partial x_{j}}, \\quad i = 1,\\ldots, n, \\quad j = 1,\\ldots,m$ is the Jacobian matrix. Often we will really only want the \"action\" of the Jacobian on a vector, so we will just want to compute the matrix-vector product $Jp$ for some vector $p$. Using the same logic, the reverse mode actually computes the Jacobian-transpose-vector product $J^{T}p$.\n",
    "\n",
    "Automatic differentiation can be used to compute derivatives to machine precision of functions $f:\\mathbb{R}^{m} \\to \\mathbb{R}^{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use autodiff32\n",
    "### Installation\n",
    "\n",
    "**1) Create a virtual environment (optional)**\n",
    "\n",
    "From the terminal, create a virtual environment:\n",
    "\n",
    "_(The command below will create the virtual environment in your present working directory, so consider moving to a project folder or a known location before creating the environment)_\n",
    "\n",
    "```virtualenv env```\n",
    "\n",
    "activate the virtual environment:\n",
    "\n",
    "```source env/bin/activate```\n",
    "\n",
    "if you plan to launch a jupyter notebook using this virtual environment, run the following to install and set up jupyter in your virtual environment:\n",
    "\n",
    "```python -m pip install jupyter```\n",
    "\n",
    "```python -m ipykernel install --user --name=env```\n",
    "\n",
    "**3) Install the autodiff32 package**\n",
    "\n",
    "In the terminal, type:\n",
    "```pip install autodiff32```\n",
    "Package dependencies will be taken care of automatically!\n",
    "\n",
    "_(Alternatively, it is also possible to install the autodiff32 package by downloading this GitHub repository. If you choose that method, use the requirements.txt file to ensure you have installed all necessary dependencies.)_\n",
    "\n",
    "## Tutorial\n",
    "\n",
    "It is easy to use the autodiff32 package in a Jupyter notebook, as we will demonstrate here:\n",
    "\n",
    "_(Alternatively, you can start a Python interpreter by typing ```Python``` into the terminal, or work from your favorite Python IDE.)_\n",
    "\n",
    "_(Remember, if you are using a virtual environment, follow steps 1 through 3 above and then type ```jupyter notebook``` into your terminal to launch a notebook. Inside the notebook, switch the kernel to that of your virtual environment.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derivative: 1.7071067811865475\n",
      "value: 5.970019778335983\n"
     ]
    }
   ],
   "source": [
    "import autodiff32 as ad\n",
    "import math # only necessary for this particular example\n",
    "\n",
    "\"\"\"\n",
    "Initialize an AutoDiff object  \n",
    "with the number you would like to pass into your function\n",
    "\"\"\"\n",
    "X = ad.AutoDiff(math.pi)\n",
    "\n",
    "\"\"\"\n",
    "Define your function of interest\n",
    "\"\"\"\n",
    "func = X + 4*ad.sin(X/4)\n",
    "\"\"\"\n",
    "Look at the derivative of your function \n",
    "evaluated at the number you gave above\n",
    "\"\"\"\n",
    "print(\"derivative:\",func.der)\n",
    "\n",
    "\"\"\"\n",
    "Look at the value of your function \n",
    "evaluated at the number you gave:\n",
    "\"\"\"\n",
    "print(\"value:\", func.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is the same equation used in our example above: $f(x) = x + 4(sin(\\frac{x}{4}))$. Just for fun, let's see if the derivative that we calculated in the evolution trace is the same as the result using autodiff32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autodiff32 derivative: 1.7071067811865475\n",
      "evolution trace derivative: 1.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "print(\"autodiff32 derivative:\", func.der)\n",
    "print(\"evolution trace derivative:\", 1+math.sqrt(2)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the derivative calculated using autodiff32 is the same as the derivative calulated by walking manually through the evolution trace!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if your function if interest has a **vector input** (X has more than one value)? In that case, use the following workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: [1 4 9]\n",
      "derivative: [2. 4. 6.]\n"
     ]
    }
   ],
   "source": [
    "X = ad.AutoDiff(np.array([1,2,3]))\n",
    "func = X**2\n",
    "print(\"value:\", func.val)\n",
    "print(\"derivative:\",func.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are three values and three derivatives. This is because your function and its derivative have been evaluated at the three values you provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if your function if interest is a **multivariate function** (has more than just an X variable)? In that case, use the following workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 16\n",
      "derivative: [4. 3.]\n"
     ]
    }
   ],
   "source": [
    "X,Y = ad.Multi_AutoDiff_Creator(X = 2, Y = 4).Vars\n",
    "func = X**2 + 3*Y\n",
    "print(\"value:\", func.val)\n",
    "print(\"derivative:\",func.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the derivative has two values. This is the derivative of your function with respect to X and Y, evaluated at the values of X and Y you provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if you actually have **multiple functions of interest**? In that case, use the following workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of first function: 6\n",
      "derivative of first function: [1. 1.]\n",
      "\n",
      "value of second function: 16\n",
      "derivative of second function: [8. 4.]\n",
      "\n",
      "Jacobian matrix:\n",
      " [[1. 1.]\n",
      " [8. 4.]]\n"
     ]
    }
   ],
   "source": [
    "X,Y = ad.Multi_AutoDiff_Creator(X = 2, Y = 4).Vars\n",
    "func = np.array([X+Y, 2*X*Y]) # two functions!\n",
    "\n",
    "# get value and derivatives of function separately\n",
    "print(\"value of first function:\", func[0].val)\n",
    "print(\"derivative of first function:\",func[0].der)\n",
    "\n",
    "print(\"\\nvalue of second function:\", func[1].val)\n",
    "print(\"derivative of second function:\",func[1].der)\n",
    "\n",
    "# return the Jacobian matrix\n",
    "J = ad.Jacobian(func)\n",
    "print(\"\\nJacobian matrix:\\n\",J.value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you have an additional option here! You can return the values and derivatives of the functions as you would normally (except that you indicate the index of the function when asking for the value or derivative), _or_ you can return the **Jacobian matrix** which contains the derivatives with respect to X and Y for each of the functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if your function if interest is a **multivariate function AND it has vector inputs**? In that case, use the following workflow:\n",
    "\n",
    "_Please note that the workflow here is significantly different from the rest of the package!_\n",
    "\n",
    "_This is due to the complexities of handling the derivatives of vector inputs for multivariate functions. We wanted to give you, the user, as much functionality as possible, even if that meant sacrificing a bit in user-friendliness._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'autodiff32' has no attribute 'MultiVarVector_AutoDiff_Evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1590a433f682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# find the values and derivatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mValues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDerivatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiVarVector_AutoDiff_Evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVarValues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"values:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'autodiff32' has no attribute 'MultiVarVector_AutoDiff_Evaluate'"
     ]
    }
   ],
   "source": [
    "# For a single multivariate function evaluated at vector value inputs\n",
    "\n",
    "# define your variables and their vector values\n",
    "X = [1,2,3]\n",
    "Y = [2,3,3]\n",
    "Z = [3,5,3]\n",
    "W = [3,5,3]\n",
    "\n",
    "# put them together in a list, in the order they will be used in your function!\n",
    "VarValues = [X, Y, Z, W]\n",
    "\n",
    "# define your function\n",
    "# Vars[0] represents X, Vars[1] represents Y, etc.\n",
    "func = lambda Vars:3*Vars[0] + 4*Vars[1] + 4*Vars[2]**2 + 3*Vars[3]\n",
    "\n",
    "# find the values and derivatives\n",
    "Values, Derivatives = ad.MultiVarVector_AutoDiff_Evaluate(VarValues,func)\n",
    "\n",
    "print(\"values:\\n\", Values)\n",
    "print(\"\\nderivatives:\\n\", Derivatives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if you have **multiple multivariate functions of interest with vector inputs**? In that case, use the following workflow:\n",
    "\n",
    "_Please note that the workflow here is significantly different from the rest of the package!_\n",
    "\n",
    "_This is due to the complexities of handling the derivatives of vector inputs for multivariate functions. We wanted to give you, the user, as much functionality as possible, even if that meant sacrificing a bit in user-friendliness._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a single multivariate function evaluated at vector value inputs\n",
    "\n",
    "# define your variables and their vector values\n",
    "X = [1,2,3]\n",
    "Y = [2,3,3]\n",
    "Z = [3,5,3]\n",
    "W = [3,5,3]\n",
    "\n",
    "# put them together in a list, in the order they will be used in your function!\n",
    "VarValues = [X, Y, Z, W]\n",
    "\n",
    "# define your functions\n",
    "# Vars[0] represents X, Vars[1] represents Y, etc.\n",
    "func = lambda Vars:np.array([3*Vars[0] + 4*Vars[1] + 4*Vars[2]**2 + 3*Vars[3],    # first function\n",
    "                                     5*Vars[0] + 6*Vars[1] + 7*Vars[2]**2 + 1*Vars[3]])   # second function\n",
    "# find the values and derivatives\n",
    "Values, Derivatives = ad.MultiVarVector_AutoDiff_Evaluate(VarValues,func)\n",
    "\n",
    "print(\"values:\\n\", Values)\n",
    "print(\"\\nderivatives:\\n\", Derivatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *TO DO: update for extension - include how to use extension as well*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "### Directory Structure\n",
    "Our structure is as follows:\n",
    "\n",
    "    /cs207-FinalProject\n",
    "        README.md\n",
    "        LICENSE\n",
    "        .gitignore\n",
    "        .travis.yml\n",
    "        setup.py\n",
    "        requirements.txt\n",
    "        docs/\n",
    "            milestone1.ipynb\n",
    "            milestone2.ipynb\n",
    "            documentation.ipynb\n",
    "        autodif32/\n",
    "            __init__.py\n",
    "            AutoDiffObj.py\n",
    "            Elementary.py\n",
    "            ElementaryReverse.py\n",
    "            Graph.py\n",
    "            JacobianVectorFunc.py\n",
    "            MultivariateVarCreator.py\n",
    "            MultivariateVectorAutoDiffEvaluate.py\n",
    "            MultivariateVectorVarCreator.py\n",
    "            ReverseJacobVectorFunc.py\n",
    "            ReverseMode.py\n",
    "        test/\n",
    "            __init__.py\n",
    "            autodiffobj_test.py\n",
    "            elementary_test.py\n",
    "            Reverse_test.py\n",
    "            JacobianVectorFunc_test.py\n",
    "            \n",
    "# *TO DO: update when finished*\n",
    "\n",
    "### Modules\n",
    "\n",
    "The ```AutoDiffObj``` module creates an AutoDiff object based on the scalar value you would like to evaluate a function and its derivative at. It overloads the basic operations including multiply, add, negative, subtract, powers, division, and equality. It also includes a Jacobian method, which returns the Jacobian of the function. If the function is univariate, then the Jacobian is just the derivative. If the function is multivariate (not yet implemented), then the Jacobian will be an array.\n",
    "\n",
    "The ```Elementary``` module implements some of the elementary functions, including exp, log, sqrt, sin, cos, tan, asin, acos, atan, sinh, cosh and tanh.\n",
    "\n",
    "The ```JacobianVectorFunc``` module implements the Jacobian method for vector inputs.\n",
    "\n",
    "The ```MultivariateVarCreator``` module takes in the values of each variable in a user defined multivariate function, and returns len(kwargs) number of AutoDiff class variables with derivative (seed) as an np.array. The MultivariateVarCreator class acts as a helper function for user to create multiple AutoDiff Objects conveniently (instead of manually create many AutoDiff objects) to use in the evaluation of the multivariate function. Please note that the implementation of multivariate functions is still in progress.\n",
    "\n",
    "# *TO DO: update when finished*\n",
    "\n",
    "### Testing Suite\n",
    "Our testing files live in the `test/` directory. The tests are run using pytest.\n",
    "\n",
    "### Installation procedure\n",
    "# *TO DO: define installation for consumers vs developers (Erica attempted this below)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation procedure\n",
    "\n",
    "1) For general users, install the autodiff32 package using pip (see 'How to Use autodiff32' above for complete instructions)\n",
    "\n",
    "```pip install autodiff32```\n",
    "\n",
    "2) For developers, feel free to clone this repository, and use the requirements.txt file to ensure you have installed all necessary dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation details\n",
    "\n",
    "The current implementation of AutoDiff32 allows for scalar univariate inputs to functions. Core classes include the AutoDiff class, . AutoDiff32 is externally dependent on numpy, and this dependency has been automatically taken care of in the released version of the package (as well as in the requirements.txt file if the user chooses to manually download the package). AutoDiff32 has implemented a number of elementary functions, as listed above in the description of the basic modules. \n",
    "\n",
    "We plan to continue development of the AutoDiff32 package to allow for vector inputs as well as multivariate inputs. This will require robust Jacobian matrix functionality. \n",
    "\n",
    "In addition to the currently implemented forward mode, we plan to build out the reverse mode of auto differentiation as an advanced feature.\n",
    "\n",
    "# *TO DO: update with extension *\n",
    "\n",
    "### Details:\n",
    "\n",
    "The core data structure (and also external dependencies) for our implementation will be numpy arrays, and the core classes we have implemented are described below:\n",
    "\n",
    "1) An ***AutoDiff class*** which stores the current value and derivative for the current node. The class method will conatin overloading operators such as plus, minus, multiply, etc.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Initialize an Automatic Differentiation object which stores its current value and derivative\n",
    "\n",
    "Note that the derivative needs to not be a scalar value\n",
    "\n",
    "For multivariable differentiation problems of a scalar function, the derivative will be a vector\n",
    "\"\"\"\n",
    "\n",
    "def __init__(self,value,der=1) :\n",
    "  self.val = val\n",
    "  self.der = der\n",
    "  \n",
    "def check_dimension(self):\n",
    "  \"\"\"\n",
    "  This method will be integrated in other class methods as well to check the dimensions when performing vector computations and catch appropriate errors\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  This method has not been implemented yet\n",
    "  \"\"\"\n",
    "\n",
    "#overloading operators to enable basic operations between classes and numbers (not exhaustive):\n",
    "\n",
    "\"\"\"\n",
    "These methods will differentiate cases in which other is a scalar, a vector, a class, or any child class\n",
    "\n",
    "All of these operators will return AutoDiff classes\n",
    "\"\"\"\n",
    "def __mult__(self,other):\n",
    "def __rmult__(self,other):\n",
    "def __radd__(self,other): \n",
    "def __add__(self,other):\n",
    "  \n",
    "# compute Jacobian for the multivariate function\n",
    "def Jacobian(self):\n",
    "  return self.der\n",
    "\n",
    "\n",
    "#For univariate functions\n",
    "X = AutoDiff(3)\n",
    "func = 3*X + 2\n",
    "print(func.val)\n",
    "print(func.der)\n",
    "\n",
    "```\n",
    "\n",
    "Multivariate functions (both scalar and vector) can be also evaluated using the AutoDiff class as below, and the resulting Jacobian will be a vector array:\n",
    "\n",
    "```python\n",
    "X = AutoDiff(1,np.array([1,0]))\n",
    "Y =  AutoDiff(1,np.array([0,1]))\n",
    "\n",
    "func = X + 2*Y\n",
    "```\n",
    "\n",
    "While this way of defining and evaluating multivariate functions is feasible, it is very inconvenient for the users to have to keep track of the dimensionality of the derivatives. For,example, the above func definition will raise an error if Y's derivative is defined as np.array([0,0,1]). This potential problem in dimensionality will also be a cause difficulties in the error handling process in the code. \n",
    "\n",
    "The way we tackle this problem is to create a helper class called **Multi_AutoDiff_Creator** as described below:\n",
    "\n",
    "2) A ***Multi_AutoDiff_Creator class*** which helps the user create variable objects from a multivariable function\n",
    "\n",
    "```python\n",
    "\n",
    "\"\"\"\n",
    "This class helps users initialize different variables from a multivariable function without explicitly specifying them using separate AutoDiff classes\n",
    "\n",
    "It will need to import AutoDiff Class\n",
    "\"\"\"\n",
    "\tdef __init__(self,*args,**kwargs):\n",
    "\t\tderi = np.identity(len(kwargs))\n",
    "\t\ti = 0 \n",
    "\t\tself.Vars =[]\n",
    "\t\tfor key, value in kwargs.items():\n",
    "\t\t\tself.List.append(AutoDiff(value,deri[i]))\n",
    "\t\t\ti+=1\n",
    "\n",
    "#Demo and comparison\n",
    "\n",
    "'''initiate a multivariable function using Multi_AutoDiff_Creator class'''\n",
    "X,Y= Multi_AutoDiff_Creator(X = 1., Y=3.).Vars\n",
    "func = X + 2*Y*X \n",
    "```\n",
    "\n",
    "Notice that this class only serves as a helper class to ensure that every variable created has the correct format of dimensionality. The class itself has no influence on the forward mode differentation process.\n",
    "\n",
    "For better calculation of the derivatives of our elementary functions, we also introduce our elementary function methods.\n",
    "\n",
    "3) An ***Elementary function*** file which calculate derivatives for elementary functions as described previously\n",
    "```python  \n",
    "    #Elementary Function Derivative (not exhaustive):\n",
    "    def exp(self,ADobj):\n",
    "    def tan(self,ADobj):\n",
    "    def sin(self,ADobj):\n",
    "    def cos(self.ADobj):\n",
    "# ...\n",
    "\n",
    "```\n",
    "\n",
    "4) A ***Jacobian*** class which helps the user compute the Jacobian matrix for vector functions\n",
    "```python \n",
    "class Jacobian:\n",
    "\tdef __init__(self,vector_func):\n",
    "\t\tself.function = vector_func\n",
    "\t\tself.l = len(self.function)\n",
    "\n",
    "\tdef value(self):\n",
    "\t\tJacob=[]\n",
    "\t\tfor i in range(self.l):\n",
    "\t\t\tJacob.append(self.function[i].der)\n",
    "\t\treturn np.array(Jacob)\n",
    "\n",
    "\n",
    "    x, y = ad.Multi_AutoDiff_Creator(x=2, y=3).Vars\n",
    "\n",
    "    #Deifne a vector function\n",
    "    func = np.array([x+y, 2*x*y])\n",
    "    Jacob = ad.Jacobian(func)\n",
    "    print(Jacob.value()) # this class method output the full jacobian matrix as an np array\n",
    "```\n",
    "\n",
    "5) An ***Advanced Feature Class*** which implements more advanced derivatives (such as the Hessian/Backward propagation) which inherits from the AutoDiff class.\n",
    "\n",
    "6) A ***Test*** class which tests existing module functionalities, which we have already implemented.\n",
    "\n",
    "\n",
    "**Aspects yet to be implemented:**\n",
    "\n",
    "1) A dimensionality check method in both the AutoDiff and the Jacobian class to help check the dimensionality of users' operations\n",
    "\n",
    "2) Error checking for all the trig functions such as sec, tanh, cosh, etc.\n",
    "\n",
    "3)The advanced feature of back propagation\n",
    "\n",
    "4) Other error handling issues that might occur\n",
    "\n",
    "# *TO DO: Fix from TA feedback (no code blocks, explain what elem functions return, if they modify objects, and providing more detail on multi_autodiff_creator and jacobian classesupdate with extension*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension\n",
    "\n",
    "# *TO DO: complete this \"Extension section\" - includes additiona information or background to understand- mathematical concepts and otherwise*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
