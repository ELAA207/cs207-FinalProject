{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "documentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n65CoS3YRB1L",
        "colab_type": "text"
      },
      "source": [
        "## autodiff32 Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOggaEsmRB1Q",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Differentiation, or the process of finding a derivative, is an extremely important mathematical operation with a wide range of applications. The discovery of extrema or zeros of functions is essential in any optimization problem, and the solving of differential equations is fundamental to modern science and engineering. Differentiation is essential in nearly all quantitative disciplines: physicists may take the derivative of the displacement of a moving object with respect to time in order to find the velocity of that object, and data scientists may use derivatives when optimizing weights in a neural network. \n",
        "\n",
        "Naturally, we would like to compute the derivative as accurately and efficiently as possible. Two classical methods of calculating the derivative have clear shortcomings. Symbolic differentiation (finding the derivative of a given formula with respect to a specified variable, producing a new formula as its output) will be accurate, but can be quite expensive computationally. The finite difference method ($\\frac{\\partial f}{\\partial x} = \\frac{f(x+\\epsilon)-f(x)}{\\epsilon}$ for some small $\\epsilon$) does not have this issue, but will be less precise as different values of epsilon will give different results. This brings us to automatic differentiation, a less costly and more precise approach.\n",
        "\n",
        "__Extension:__ For this project, we have implemented forward-mode automatic differentiation as well as reverse-mode automatic differentiation. It is important to have both methods accessible to have accuracy as well as optimal efficiency. \n",
        "\n",
        "Automatic differentiation can be used to compute derivatives to machine precision of functions $f:\\mathbb{R}^{m} \\to \\mathbb{R}^{n}$\n",
        "\n",
        "The forward mode is more efficient when $n\\gg m$.\n",
        "   - This corresponds to the case where the number of functions to evaluate is much greater than the number of inputs.\n",
        "   - Actually computes the Jacobian-vector product $Jp$.\n",
        "   \n",
        "The reverse mode is more efficient when $n\\ll m$.\n",
        "   - This corresponds to the case where the number of inputs is much greater than the number of functions.\n",
        "   - Actually computes the Jacobian-transpose-vector product $J^{T}p$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wktePkP2RB1R",
        "colab_type": "text"
      },
      "source": [
        "## Background\n",
        "\n",
        "Automatic differentiation breaks down the main function into elementary functions, evaluated upon one another. It then uses the chain rule to update the derivative at each step and ends in the derivative of the entire function.\n",
        "\n",
        "To better understand this process, let's look at an example. Consider the example function\n",
        "\n",
        "\\begin{equation}\n",
        "f(x) = x + 4sin(\\frac{x}{4})\n",
        "\\end{equation}\n",
        "\n",
        "We would like to compute the derivative of this function at a particular value of x. Let's say that in this case, we are interested in evaluating the derivative at $x=\\pi$. In other words, we want to find $f'(\\pi)$ where $f'(x) = \\frac{\\partial f}{\\partial x}$\n",
        "\n",
        "We know how to solve this _symbolically_ using methods that we learned in calculus, but remember, we want to compute this answer as accurately and efficiently as possible, which is why we want to solve it using automatic differentiation. \n",
        "\n",
        "### The Chain Rule\n",
        "\n",
        "To solve this using automatic differentiation, we need to find the decomposition of the differentials provied by the chain rule. Remember, the chain rule is a formula for computing the derivative of the composition of two or more functions. So if we have a function $h\\left(u\\left(t\\right)\\right)$ and we want the derivative of $h$ with respect to $t$, then we know by the chain rule that the derivative is $\\dfrac{\\partial h}{\\partial t} = \\dfrac{\\partial h}{\\partial u}\\dfrac{\\partial u}{\\partial t}.$ The chain rule can also be expanded to deal with multiple arguments and vector inputs (in which case we would be calculating the _gradient)_.\n",
        "\n",
        "Our function $f(x)$ is composed of elemental functions for which we know the derivatives. We will separate out each of these elemental functions, evaluating the derivative at each step using the chain rule. \n",
        "\n",
        "### Forward-mode differentiation\n",
        "\n",
        "Using forward-mode differentiation, the evaluation trace for this problem looks like:\n",
        "\n",
        "| Trace    | Elementary Operation | Derivative | $f'(a)$ |\n",
        "| :------: | :----------------------: | :------------------------------:  | :------------------------------: |\n",
        "| $x_{3}$  | $\\pi$ | $1$ | $\\pi$ | $1$ |\n",
        "| $x_{0}$  | $\\frac{x_{3}}{4}$ | $\\frac{\\dot{x}_{3}}{4}$ | $\\frac{1}{4}$ |\n",
        "| $x_{1}$  | $\\sin\\left(x_{0}\\right)$ | $\\cos\\left(x_{0}\\right)\\dot{x}_{0}$ | $\\frac{\\sqrt{2}}{8}$|\n",
        "| $x_{2}$  | $4x_{1}$ | $4\\dot{1}_{3}$ | $\\frac{\\sqrt{2}}{2}$ |\n",
        "| $x_{4}$  | $x_{2} + x_{3}$| $\\dot{x}_{2} + \\dot{x}_{3}$ | $1 + \\frac{\\sqrt{2}}{2}$ |\n",
        "\n",
        "This evaluation trace provides some intuition for how forward-mode automatic differentiation is used to calculate the derivative of a function evaluated at a certain value ($f'(\\pi) = 1 + \\frac{\\sqrt{2}}{2}$). \n",
        "\n",
        "<img src=\"computational_graph.png\"></img>\n",
        "*Figure 1: Forward-mode computational graph for example above*\n",
        "\n",
        "### Reverse-mode automatic differentiation\n",
        "\n",
        "Using reverse-mode differentiation, sometimes called backpropagation, the trace on the right is evaluated top to bottom intsead. \n",
        "\n",
        "| Trace    | Elementary Operation | Derivative &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| $f'(a)$ |\n",
        "| :------: | :----------------------: | :------------------------------: | :------------------------------: |\n",
        "| $x_{3}$  | $\\pi$ | $\\frac{\\partial{x_0}}{\\partial{x_3}}\\bar{x_0} + \\bar{x_4} = \\cos(\\frac{\\pi}{4}) + 1$ | $1 + \\frac{\\sqrt{2}}{2}$ |\n",
        "| $x_{0}$  | $\\frac{x_{3}}{4}$ | $\\frac{\\partial{x_1}}{\\partial{x_0}}\\bar{x_1} = 4\\cos(\\frac{\\pi}{4})$ | $2\\sqrt{2}$ |\n",
        "| $x_{1}$  | $\\sin\\left(x_{0}\\right)$ | $\\frac{\\partial{x_2}}{\\partial{x_1}}\\bar{x_2}$| $4$|\n",
        "| $x_{2}$  | $4x_{1}$ | $\\frac{\\partial{x_4}}{\\partial{x_2}}\\bar{x_4}$ | $1$ |\n",
        "| $x_{4}$  | $x_{2} + x_{3}$| $1$ | $1$ | $1$ \n",
        "\n",
        "<img src=\"reverse_graph.png\"></img>\n",
        "*Figure 2: Reverse-mode computational graph for example above*\n",
        "\n",
        "You may notice that when we computed the derivative above, we \"seeded\" the derivative with a value of 1. This seed vector doesn't have to be 1, but the utility of using a unit vector becomes apparent when we consider a problem involving directional derivatives.\n",
        "\n",
        "The definition of the directional derivative (where $p$ is the seed vector)\n",
        "$$D_{p}x_{3} = \\sum_{j=1}^{2}{\\dfrac{\\partial x_{3}}{\\partial x_{j}}p_{j}}$$\n",
        "\n",
        "can be expanded to\n",
        "\n",
        "\\begin{align}\n",
        "  D_{p}x_{3} &= \\dfrac{\\partial x_{3}}{\\partial x_{1}}p_{1} + \\dfrac{\\partial x_{3}}{\\partial x_{2}}p_{2} \\\\\n",
        "             &= x_{2}p_{1} + x_{1}p_{2}\n",
        "\\end{align}\n",
        "\n",
        "If we choose $p$ to be a the unit vector, we can see how this is beneficial:\n",
        "\n",
        "$p = \\left(1,0\\right)$ gives $\\dfrac{\\partial f}{\\partial x}$\n",
        "\n",
        "$p = \\left(0,1\\right)$ gives $\\dfrac{\\partial f}{\\partial y}$\n",
        "\n",
        "So to summarize, the forward mode of automatic differentiation is really computing the _product of the gradient of our function with the seed vector:_\n",
        "\n",
        "$$D_{p}x_{3} = \\nabla x_{3}\\cdot p.$$ \n",
        "\n",
        "If our function is a vector, then the forward mode actually computes $Jp$ where $J = \\dfrac{\\partial f_{i}}{\\partial x_{j}}, \\quad i = 1,\\ldots, n, \\quad j = 1,\\ldots,m$ is the Jacobian matrix. Often we will really only want the \"action\" of the Jacobian on a vector, so we will just want to compute the matrix-vector product $Jp$ for some vector $p$. Using the same logic, the reverse mode actually computes the Jacobian-transpose-vector product $J^{T}p$.\n",
        "\n",
        "Automatic differentiation can be used to compute derivatives to machine precision of functions $f:\\mathbb{R}^{m} \\to \\mathbb{R}^{n}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gss6Ih33RB1S",
        "colab_type": "text"
      },
      "source": [
        "## How to Use autodiff32\n",
        "### Installation\n",
        "\n",
        "**1) Create a virtual environment (optional)**\n",
        "\n",
        "From the terminal, create a virtual environment:\n",
        "\n",
        "_(The command below will create the virtual environment in your present working directory, so consider moving to a project folder or a known location before creating the environment)_\n",
        "\n",
        "```virtualenv env```\n",
        "\n",
        "activate the virtual environment:\n",
        "\n",
        "```source env/bin/activate```\n",
        "\n",
        "if you plan to launch a jupyter notebook using this virtual environment, run the following to install and set up jupyter in your virtual environment:\n",
        "\n",
        "```python -m pip install jupyter```\n",
        "\n",
        "```python -m ipykernel install --user --name=env```\n",
        "\n",
        "**3) Install the autodiff32 package**\n",
        "\n",
        "In the terminal, type:\n",
        "```pip install autodiff32```\n",
        "Package dependencies will be taken care of automatically!\n",
        "\n",
        "_(Alternatively, it is also possible to install the autodiff32 package by downloading this GitHub repository. If you choose that method, use the requirements.txt file to ensure you have installed all necessary dependencies.)_\n",
        "\n",
        "## Tutorial\n",
        "\n",
        "It is easy to use the autodiff32 package in a Jupyter notebook, as we will demonstrate here:\n",
        "\n",
        "_(Alternatively, you can start a Python interpreter by typing ```Python``` into the terminal, or work from your favorite Python IDE.)_\n",
        "\n",
        "_(Remember, if you are using a virtual environment, follow steps 1 through 3 above and then type ```jupyter notebook``` into your terminal to launch a notebook. Inside the notebook, switch the kernel to that of your virtual environment.)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvv3m0TSq8Ok",
        "colab_type": "code",
        "outputId": "65f3bf75-0b72-457b-e166-dd28a838b9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "pip install autodiff32"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autodiff32\n",
            "  Downloading https://files.pythonhosted.org/packages/b6/8c/88fccbf0cc72968be09e75c7bd9f6811d6c675bba90d825e2fc5dfdd145e/autodiff32-0.1.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from autodiff32) (1.17.4)\n",
            "Installing collected packages: autodiff32\n",
            "Successfully installed autodiff32-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIJeO9eWRB1T",
        "colab_type": "code",
        "outputId": "c3ab1f31-70f2-446c-f257-a99fb079d994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import autodiff32 as ad\n",
        "import math # only necessary for this particular example\n",
        "\n",
        "\"\"\"\n",
        "Initialize an AutoDiff object  \n",
        "with the number you would like to pass into your function\n",
        "\"\"\"\n",
        "X = ad.AutoDiff(math.pi)\n",
        "\n",
        "\"\"\"\n",
        "Define your function of interest\n",
        "\"\"\"\n",
        "func = X + 4*ad.sin(X/4)\n",
        "\"\"\"\n",
        "Look at the derivative of your function \n",
        "evaluated at the number you gave above\n",
        "\"\"\"\n",
        "print(\"derivative:\",func.der)\n",
        "\n",
        "\"\"\"\n",
        "Look at the value of your function \n",
        "evaluated at the number you gave:\n",
        "\"\"\"\n",
        "print(\"value:\", func.val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "derivative: 1.7071067811865475\n",
            "value: 5.970019778335983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex7bVik4RB1X",
        "colab_type": "text"
      },
      "source": [
        "Notice that this is the same equation used in our example above: $f(x) = x + 4(sin(\\frac{x}{4}))$. Just for fun, let's see if the derivative that we calculated in the evolution trace is the same as the result using autodiff32:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M6ICTkJRB1X",
        "colab_type": "code",
        "outputId": "f7501d7f-9fd6-4115-ade1-3d6c21cce5b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(\"autodiff32 derivative:\", func.der)\n",
        "print(\"evolution trace derivative:\", 1+math.sqrt(2)/2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "autodiff32 derivative: 1.7071067811865475\n",
            "evolution trace derivative: 1.7071067811865475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cIZFPDrRB1a",
        "colab_type": "text"
      },
      "source": [
        "We can see that the derivative calculated using autodiff32 is the same as the derivative calulated by walking manually through the evolution trace!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYu8Zz2QRB1a",
        "colab_type": "text"
      },
      "source": [
        "Now what if your function if interest has a **vector input** (X has more than one value)? In that case, use the following workflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL7V_sB3RB1b",
        "colab_type": "code",
        "outputId": "50d94c68-0b37-4bf5-e65a-51c178b06955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "X = ad.AutoDiff(np.array([1,2,3]))\n",
        "func = X**2\n",
        "print(\"value:\", func.val)\n",
        "print(\"derivative:\",func.der)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "value: [1 4 9]\n",
            "derivative: [2. 4. 6.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyBCIUzxRB1d",
        "colab_type": "text"
      },
      "source": [
        "Notice that there are three values and three derivatives. This is because your function and its derivative have been evaluated at the three values you provided.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXmtGVIcRB1e",
        "colab_type": "text"
      },
      "source": [
        "Now what if your function if interest is a **multivariate function** (has more than just an X variable)? In that case, use the following workflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTuNzWAlRB1e",
        "colab_type": "code",
        "outputId": "4395c0f8-5cd5-42cb-cc5e-6304583faf0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X,Y = ad.Multi_AutoDiff_Creator(X = 2, Y = 4).Vars\n",
        "func = X**2 + 3*Y\n",
        "print(\"value:\", func.val)\n",
        "print(\"derivative:\",func.der)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "value: 16\n",
            "derivative: [4. 3.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMmUESrGRB1g",
        "colab_type": "text"
      },
      "source": [
        "Notice that the derivative has two values. This is the derivative of your function with respect to X and Y, evaluated at the values of X and Y you provided.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVc8Y2YURB1h",
        "colab_type": "text"
      },
      "source": [
        "Now what if you actually have **multiple functions of interest**? In that case, use the following workflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl03xRFKRB1h",
        "colab_type": "code",
        "outputId": "775578fd-0b4c-4bcb-9553-386473c740ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "X,Y = ad.Multi_AutoDiff_Creator(X = 2, Y = 4).Vars\n",
        "func = np.array([X+Y, 2*X*Y]) # two functions!\n",
        "\n",
        "# get value and derivatives of function separately\n",
        "print(\"value of first function:\", func[0].val)\n",
        "print(\"derivative of first function:\",func[0].der)\n",
        "\n",
        "print(\"\\nvalue of second function:\", func[1].val)\n",
        "print(\"derivative of second function:\",func[1].der)\n",
        "\n",
        "# return the Jacobian matrix\n",
        "J = ad.Jacobian(func)\n",
        "print(\"\\nJacobian matrix:\\n\",J.value())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "value of first function: 6\n",
            "derivative of first function: [1. 1.]\n",
            "\n",
            "value of second function: 16\n",
            "derivative of second function: [8. 4.]\n",
            "\n",
            "Jacobian matrix:\n",
            " [[1. 1.]\n",
            " [8. 4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjSQNe9IRB1k",
        "colab_type": "text"
      },
      "source": [
        "Notice that you have an additional option here! You can return the values and derivatives of the functions as you would normally (except that you indicate the index of the function when asking for the value or derivative), _or_ you can return the **Jacobian matrix** which contains the derivatives with respect to X and Y for each of the functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md5dfRqERB1k",
        "colab_type": "text"
      },
      "source": [
        "Now what if your function if interest is a **multivariate function AND it has vector inputs**? In that case, use the following workflow:\n",
        "\n",
        "_Please note that the workflow here is significantly different from the rest of the package!_\n",
        "\n",
        "_This is due to the complexities of handling the derivatives of vector inputs for multivariate functions. We wanted to give you, the user, as much functionality as possible, even if that meant sacrificing a bit in user-friendliness._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMwFjLWdRB1l",
        "colab_type": "code",
        "outputId": "e5ab6fb6-47a9-4cc0-f964-bd195167c5be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "# For a single multivariate function evaluated at vector value inputs\n",
        "\n",
        "# define your variables and their vector values\n",
        "X = [1,2,3]\n",
        "Y = [2,3,3]\n",
        "Z = [3,5,3]\n",
        "W = [3,5,3]\n",
        "\n",
        "# put them together in a list, in the order they will be used in your function!\n",
        "VarValues = [X, Y, Z, W]\n",
        "\n",
        "# define your function\n",
        "# Vars[0] represents X, Vars[1] represents Y, etc.\n",
        "func = lambda Vars:3*Vars[0] + 4*Vars[1] + 4*Vars[2]**2 + 3*Vars[3]\n",
        "\n",
        "# find the values and derivatives\n",
        "Values, Derivatives = ad.MultiVarVector_AutoDiff_Evaluate(VarValues,func)\n",
        "\n",
        "print(\"values:\\n\", Values)\n",
        "print(\"\\nderivatives:\\n\", Derivatives)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values:\n",
            " [ 56 133  66]\n",
            "\n",
            "derivatives:\n",
            " [[ 3.  4. 24.  3.]\n",
            " [ 3.  4. 40.  3.]\n",
            " [ 3.  4. 24.  3.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLBdpOrXRB1n",
        "colab_type": "text"
      },
      "source": [
        "Now what if you have **multiple multivariate functions of interest with vector inputs**? In that case, use the following workflow:\n",
        "\n",
        "_Please note that the workflow here is significantly different from the rest of the package!_\n",
        "\n",
        "_This is due to the complexities of handling the derivatives of vector inputs for multivariate functions. We wanted to give you, the user, as much functionality as possible, even if that meant sacrificing a bit in user-friendliness._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkxKjLRZRB1n",
        "colab_type": "code",
        "outputId": "6f27bd72-4317-487b-e0ea-c09f071c783e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# For a single multivariate function evaluated at vector value inputs\n",
        "\n",
        "# define your variables and their vector values\n",
        "X = [1,2,3]\n",
        "Y = [2,3,3]\n",
        "Z = [3,5,3]\n",
        "W = [3,5,3]\n",
        "\n",
        "# put them together in a list, in the order they will be used in your function!\n",
        "VarValues = [X, Y, Z, W]\n",
        "\n",
        "# define your functions\n",
        "# Vars[0] represents X, Vars[1] represents Y, etc.\n",
        "func = lambda Vars:np.array([3*Vars[0] + 4*Vars[1] + 4*Vars[2]**2 + 3*Vars[3],    # first function\n",
        "                                     5*Vars[0] + 6*Vars[1] + 7*Vars[2]**2 + 1*Vars[3]])   # second function\n",
        "# find the values and derivatives\n",
        "Values, Derivatives = ad.MultiVarVector_AutoDiff_Evaluate(VarValues,func)\n",
        "\n",
        "print(\"values:\\n\", Values)\n",
        "print(\"\\nderivatives:\\n\", Derivatives)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "values:\n",
            " [[ 56  83]\n",
            " [133 208]\n",
            " [ 66  99]]\n",
            "\n",
            "derivatives:\n",
            " [[[ 3.  4. 24.  3.]\n",
            "  [ 5.  6. 42.  1.]]\n",
            "\n",
            " [[ 3.  4. 40.  3.]\n",
            "  [ 5.  6. 70.  1.]]\n",
            "\n",
            " [[ 3.  4. 24.  3.]\n",
            "  [ 5.  6. 42.  1.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Th2N8rRVI-",
        "colab_type": "text"
      },
      "source": [
        "**Extension Usage Demo**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbKFkDIAiveP",
        "colab_type": "code",
        "outputId": "4b09d34c-9fe5-42a6-ccbb-a48db5cce19c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        " \n",
        "# For univariate /Multivariate Scalar Function\n",
        "Graph = ad.ComputationalGraph()\n",
        "X = ad.Node(value = 3, Graph = Graph)\n",
        "Y = ad.Node(value = 4, Graph = Graph)\n",
        "Z = ad.Node(value = 1, Graph = Graph)\n",
        "\n",
        "G = 2*X + 3*Y*Z + 2*Z\n",
        "Graph.ComputeValue()\n",
        "Graph.ComputeGradient(-1)\n",
        "print(\"Derivative for X is: \",X.deri)\n",
        "print(\"Derivative for Y is: \",Y.deri)\n",
        "print(\"Derivative for Z is: \",Z.deri)\n",
        "print(\"Value of Function is: \",G.value)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Derivative for X is:  2.0\n",
            "Derivative for Y is:  3.0\n",
            "Derivative for Z is:  14.0\n",
            "Value of Function is:  20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVAr5ESTFiAi",
        "colab_type": "code",
        "outputId": "afa22b98-fb3f-4f65-aa12-2515d3c12617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "#For univariate or Multivariate Vector Functions with single value\n",
        "import numpy as np\n",
        "Graph = ad.ComputationalGraph()\n",
        "X = ad.Node(value = 3, Graph = Graph)\n",
        "Y = ad.Node(value = 4, Graph = Graph)\n",
        "Z = ad.Node(value = 1, Graph = Graph)\n",
        "\n",
        "G = np.array([-2*ad.sinr(X),  #please use sinr for sin operation on the node\n",
        "               2*Y + Z*Y,\n",
        "               3*X+3*Y*X+2*Z])\n",
        "\n",
        "Func = ad.ReverseVecFunc(G,X =X,Y= Y,Z= Z)\n",
        "Value ,Derivative = Func.value(Graph)\n",
        "print(\"The value for the vector function is: \")\n",
        "print(Value)\n",
        "print(\"The derivative for the vector function is: \")\n",
        "print(Derivative)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The value for the vector function is: \n",
            "[-0.28224002 12.         47.        ]\n",
            "The derivative for the vector function is: \n",
            "[[ 1.97998499  0.          0.        ]\n",
            " [ 0.          3.          4.        ]\n",
            " [15.          9.          2.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLRVyqUWGlTU",
        "colab_type": "code",
        "outputId": "cd3adfdc-2c8e-42ca-9f0a-208e52350ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "#SERIES OF VALUES For vector functions\n",
        "D = 3 # number of variables\n",
        "x = [1,2,3]\n",
        "y = [6,7,4]\n",
        "z = [3,8,1]\n",
        "Values = np.array([x,y,z])\n",
        "G = np.array([-2*X,  #please use sinr for sin operation on the node\n",
        "               2*Y + Z*Y,\n",
        "               3*X+3*Y*X+2*Z])\n",
        "Func = ad.ReverseVecFunc(G,X =X,Y= Y,Z= Z)\n",
        "Vals,Deris=Func.Seriesvalue(Values,D,Graph)\n",
        "print(\"The value for the vector function is: \")\n",
        "print(Vals)\n",
        "print(\"The derivative for the vector function is: \")\n",
        "print(np.array(Deris))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The value for the vector function is: \n",
            "[[-2 30 27]\n",
            " [-4 70 64]\n",
            " [-6 12 47]]\n",
            "The derivative for the vector function is: \n",
            "[[[-2.  0.  0.]\n",
            "  [ 0.  5.  6.]\n",
            "  [21.  3.  2.]]\n",
            "\n",
            " [[-2.  0.  0.]\n",
            "  [ 0. 10.  7.]\n",
            "  [24.  6.  2.]]\n",
            "\n",
            " [[-2.  0.  0.]\n",
            "  [ 0.  3.  4.]\n",
            "  [15.  9.  2.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw54CVb4SfQV",
        "colab_type": "code",
        "outputId": "ae3bb0e5-bd80-447e-f789-8407456d6262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "Graph = ad.ComputationalGraph()\n",
        "X = ad.Node(value = 3, Graph = Graph)\n",
        "F = 3*X**2\n",
        "Xvals= np.array([[3,2,4]]) #Please input Xvals as a 2 dimensional array\n",
        "Vals, deri = Graph.SeriesValues(Xvals,1,Graph)\n",
        "print(Vals)\n",
        "print(deri)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[27, 12, 48]\n",
            "[[18.0], [12.0], [24.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORwjEJWKRB1q",
        "colab_type": "text"
      },
      "source": [
        "## Software Organization\n",
        "### Directory Structure\n",
        "Our structure is as follows:\n",
        "\n",
        "    /cs207-FinalProject\n",
        "        README.md\n",
        "        LICENSE\n",
        "        .gitignore\n",
        "        .travis.yml\n",
        "        setup.py\n",
        "        requirements.txt\n",
        "        docs/\n",
        "            milestone1.ipynb\n",
        "            milestone2.ipynb\n",
        "            documentation.ipynb\n",
        "        autodif32/\n",
        "            __init__.py\n",
        "            AutoDiffObj.py\n",
        "            Elementary.py\n",
        "            ElementaryReverse.py\n",
        "            Graph.py\n",
        "            JacobianVectorFunc.py\n",
        "            MultivariateVarCreator.py\n",
        "            MultivariateVectorAutoDiffEvaluate.py\n",
        "            MultivariateVectorVarCreator.py\n",
        "            ReverseJacobVectorFunc.py\n",
        "            ReverseMode.py\n",
        "        test/\n",
        "            __init__.py\n",
        "            autodiffobj_test.py\n",
        "            elementary_test.py\n",
        "            Reverse_test.py\n",
        "            JacobianVectorFunc_test.py\n",
        "            \n",
        "\n",
        "### Modules\n",
        "\n",
        "The ```AutoDiffObj``` module creates an AutoDiff object based on the scalar value you would like to evaluate a function and its derivative at. It overloads the basic operations including multiply, add, negative, subtract, powers, division, and equality. It also includes a Jacobian method, which returns the Jacobian of the function. If the function is univariate, then the Jacobian is just the derivative. If the function is multivariate (not yet implemented), then the Jacobian will be an array.\n",
        "\n",
        "The ```Elementary``` module implements some of the elementary functions, including exp, log, sqrt, sin, cos, tan, asin, acos, atan, sinh, cosh and tanh.\n",
        "\n",
        "The ```JacobianVectorFunc``` module implements the Jacobian method for vector inputs.\n",
        "\n",
        "The ```MultivariateVarCreator``` module takes in the values of each variable in a user defined multivariate function, and returns len(kwargs) number of AutoDiff class variables with derivative (seed) as an np.array. The MultivariateVarCreator class acts as a helper function for user to create multiple AutoDiff Objects conveniently (instead of manually create many AutoDiff objects) to use in the evaluation of the multivariate function. Please note that the implementation of multivariate functions is still in progress.\n",
        "\n",
        "### Testing Suite\n",
        "Our testing files live in the `test/` directory. The tests are run using pytest.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh3FSEUQRB1r",
        "colab_type": "text"
      },
      "source": [
        "### Installation procedure\n",
        "\n",
        "1) For general users, install the autodiff32 package using pip (see 'How to Use autodiff32' above for complete instructions)\n",
        "\n",
        "```pip install autodiff32```\n",
        "\n",
        "2) For developers, feel free to clone this repository, and use the requirements.txt file to ensure you have installed all necessary dependencies. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgFKv5mRB1r",
        "colab_type": "text"
      },
      "source": [
        "## Implementation details\n",
        "\n",
        "The current implementation of AutoDiff32 allows for scalar univariate inputs to functions. Core classes include the AutoDiff class, . AutoDiff32 is externally dependent on numpy, and this dependency has been automatically taken care of in the released version of the package (as well as in the requirements.txt file if the user chooses to manually download the package). AutoDiff32 has implemented a number of elementary functions, as listed above in the description of the basic modules. \n",
        "\n",
        "We plan to continue development of the AutoDiff32 package to allow for vector inputs as well as multivariate inputs. This will require robust Jacobian matrix functionality. \n",
        "\n",
        "In addition to the currently implemented forward mode, we plan to build out the reverse mode of auto differentiation as an advanced feature.\n",
        "\n",
        "\n",
        "\n",
        "### Forward mode Details:\n",
        "\n",
        "The core data structure (and also external dependencies) for our implementation will be numpy arrays, and the core classes we have implemented are described below:\n",
        "\n",
        "1) An ***AutoDiff class*** which stores the current value and derivative for the current node. The class method will conatin overloading operators such as plus, minus, multiply, etc.\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Initialize an Automatic Differentiation object which stores its current value and derivative\n",
        "\n",
        "Note that the derivative needs to not be a scalar value\n",
        "\n",
        "For multivariable differentiation problems of a scalar function, the derivative will be a vector\n",
        "\"\"\"\n",
        "\n",
        "def __init__(self,value,der=1) :\n",
        "  #Store the value of the Autodiff object\n",
        "  #Store derivative of autodiff object (default value is 1)\n",
        "\n",
        "\n",
        "#overloading operators to enable basic operations between classes and numbers (not exhaustive):\n",
        "\n",
        "\"\"\"\n",
        "These methods will differentiate cases in which other is a scalar, a vector, a class, or any child class\n",
        "\n",
        "All of these operators will return AutoDiff classes\n",
        "\"\"\"\n",
        "def __mult__(self,other):\n",
        "def __rmult__(self,other):\n",
        "def __radd__(self,other): \n",
        "def __add__(self,other):\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "Multivariate functions (both scalar and vector) can be also evaluated using the AutoDiff class as below, and the resulting Jacobian will be a vector array:\n",
        "\n",
        "```python\n",
        "X = AutoDiff(1,np.array([1,0]))\n",
        "Y =  AutoDiff(1,np.array([0,1]))\n",
        "\n",
        "func = X + 2*Y\n",
        "```\n",
        "\n",
        "While this way of defining and evaluating multivariate functions is feasible, it is very inconvenient for the users to have to keep track of the dimensionality of the derivatives. For,example, the above func definition will raise an error if Y's derivative is defined as np.array([0,0,1]). This potential problem in dimensionality will also be a cause difficulties in the error handling process in the code. \n",
        "\n",
        "The way we tackle this problem is to create a helper class called **Multi_AutoDiff_Creator** as described below:\n",
        "\n",
        "2) A ***Multi_AutoDiff_Creator class*** which helps the user create variable objects from a multivariable function\n",
        "\n",
        "```python\n",
        "\n",
        "\"\"\"\n",
        "This class helps users initialize different variables from a multivariable function without explicitly specifying them using separate AutoDiff classes\n",
        "\n",
        "It will need to import AutoDiff Class\n",
        "\"\"\"\n",
        "\tdef __init__(self,*args,**kwargs):\n",
        "    '''\n",
        "    INPUT : variables as kwargs such as (X=3,Y =4)\n",
        "    RETURN : X number of autodiff objects with length kwargs and each with its 'vector' derivatives\n",
        "\n",
        "    '''\n",
        "    #Demo and comparison\n",
        "\n",
        "    '''initiate a multivariable function using Multi_AutoDiff_Creator class'''\n",
        "    X,Y= Multi_AutoDiff_Creator(X = 1., Y=3.).Vars #X,Y are autodiff object with derivative [1,0] and [0,1]\n",
        "    func = X + 2*Y*X \n",
        "```\n",
        "\n",
        "Notice that this class only serves as a **helper class** to ensure that every variable created has the correct format of dimensionality. The class itself has no influence on the forward mode differentation process.\n",
        "\n",
        "For better calculation of the derivatives of our elementary functions, we also introduce our elementary function methods.\n",
        "\n",
        "3) An ***Elementary function*** file which calculate derivatives for elementary functions as described previously\n",
        "```python  \n",
        "    #Elementary Function Derivative (not exhaustive):\n",
        "    def exp(self,ADobj):\n",
        "    def tan(self,ADobj):\n",
        "    def sin(self,ADobj):\n",
        "    def cos(self.ADobj):\n",
        "    '''\n",
        "   RETURN an AutoDiff object if ADobj is an AutoDiff object with related value and derivative for the particular elementary functions\n",
        "   Return np.exp/tan/sin(ODobj) if ODobj is a number\n",
        "    '''\n",
        "\n",
        "```\n",
        "\n",
        "4) A ***Jacobian*** class which helps the user compute the Jacobian matrix for vector functions evaluated at a single point\n",
        "```python \n",
        "class Jacobian:\n",
        "\tdef __init__(self,vector_func):\n",
        "\t\t#The Jacobian class is initiated by a vector function\n",
        "\n",
        "\tdef value(self):\n",
        "    # Return the Jacobian value of the vector functions evaluated at a single point by looping through the vector function\n",
        "    '''\n",
        "      x, y = ad.Multi_AutoDiff_Creator(x=2, y=3).Vars\n",
        "      #Define a vector function\n",
        "      func = np.array([x+y, 2*x*y])\n",
        "      Jacob = ad.Jacobian(func)\n",
        "      print(Jacob.value()) # this class method output the full jacobian matrix as an np array\n",
        "      '''\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtfrL-oGRB1s",
        "colab_type": "text"
      },
      "source": [
        "## Extension\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Pw7-62mACM",
        "colab_type": "text"
      },
      "source": [
        "**Description**\n",
        "\n",
        "The implementation of reverse mode also lies in the same autodiff32 package with no additional extension requirements.\n",
        "\n",
        "We implemented the reverse mode of automatic differentiation in which users can evaluate univariate and multivariate scalar/vector functions with series or single values. The alogorithm is based on the computational flow chart(graph) for reverse mode discussed inclass in which each node  is recorded in sequence in a Graph later in use of value and derivative computation. In particular, each node stored its value, teh graph it connects to, and the index it has in the graph. In order to backward compute the deriavtive for our root node, each node also records the parent from which produce it through some operations, and also the oepration code (\"plus\",\"sub\",etc.) itself. \n",
        "\n",
        "We find this more intuitive and pedagogical than using recursion, and potentially can be more computationally efficient since each node only has to remeber its direct children but not the indirect ones. The graph and node can also be reused easily, which makes it simpler in our evaluation for multiple values of our variables.\n",
        "\n",
        "The high level mathematical ideas will be similar to that of the forward mode, the only additional thing that will be helpful to keep in mind is the chain rule, which is:\n",
        "\n",
        "\\begin{aligned}\n",
        "\\frac{dz}{dx} = \\frac{dz}{dy}\\times\\frac{dy}{dx}\n",
        "\\end{aligned}\n",
        "\n",
        "This will help use understand how we can calculate the derivatives by starting to set seed value of 1 for our function, which is:\n",
        "\\begin{aligned}\n",
        "\\frac{dz}{df} = 1\n",
        "\\end{aligned}\n",
        "For example, if we would like to calculate the derivative for $f = 2x$ backward,by letting $z=f$,we will have:\n",
        "\n",
        "\\begin{aligned}\n",
        "\\frac{dz}{dx}=\\frac{dz}{df}\\times\\frac{df}{dx} = 1 \\times 2 = 2  \n",
        "\\end{aligned}\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOzf0Yfo9Tqy",
        "colab_type": "text"
      },
      "source": [
        "###Implementation Details for Reverse Mode\n",
        "\n",
        "The reverse mode implementation will have the following classes:\n",
        "\n",
        "1) A **Node** class which serve as single automatic differentiation object.All the node will be connected to the same graph for a given function.\n",
        "```python \n",
        "class Node:\n",
        "  #INITIATOR\n",
        "  '''\n",
        "    The Node class is initiated with the paramters mentioned below. The node \n",
        "    will connect to the graph as soon as it is created.\n",
        "  '''\n",
        "  def __init__(values,Graph,derivative,leftparentnode,rightparentnode,derivative = 0):\n",
        "\n",
        "\n",
        "  def CheckConstant(x):\n",
        "    #Check if x is a Node or not. If it is ,return it,\n",
        "    # if not,return a new node with Node.value = x ,and connect it to the Graph.\n",
        "  \n",
        "  #OVERLOADING OPERATORS\n",
        "  '''\n",
        "  perform subtraction between nodes\n",
        "  Return\n",
        "  ======\n",
        "  A new node which store the value,the graph it connects to, and self as its left nod and other as its right node. \n",
        "  '''\n",
        "  def __mul__(self,other):\n",
        "  def __sub__(self,pther):\n",
        "  def __truediv__(self,other):\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "2 A **ComputationalGraph** class which stores the nodes of a given function in sequence and compute the value and the gradient of the function and the root variables.\n",
        "\n",
        "```python\n",
        "\n",
        "class ComputationalGraph:\n",
        "  \n",
        "  def __init__(self):\n",
        "    # the graph is initialized by an empty list\n",
        "  def append(self):\n",
        "    #append the node in sequence in the graph and record its index for later computation\n",
        "  def ValidOp(self):\n",
        "    # sturcturely store every valid operator in this reverse mode computation\n",
        "     '''\n",
        "    RETURN\n",
        "    ======\n",
        "    Valid operator code if valid otherwise raise error\n",
        "    '''\n",
        "  def ComputeValue(self):\n",
        "    #Compute the Value of the function by a forward pass of th graph\n",
        "\n",
        "  def ComputeGradient(self,lastindex  = -1):\n",
        "    '''\n",
        "    Backward propagate through the graph to calcutate the derivative of the  nodes and store it in each node by looping through the list in a reverse order and update the parent nodes using child nodes.\n",
        "    INPUT : Last index is the seed : dz/df = 1\n",
        "\n",
        "    RETURN\n",
        "    ======\n",
        "    NONE\n",
        "    All the values after computation is stored in each node in the list\n",
        "    '''\n",
        "  def SeriesValues(self,args):\n",
        "    #compute the value and derivatives for \n",
        "    #a series of values for a function (illustration in detail in the How to use session)\n",
        "    '''\n",
        "    RETURN\n",
        "    ===\n",
        "    A two dimensional  Array for derivatives and one dimensional array for values\n",
        "    Values for the funtion evaulated at different points\n",
        "    Derivatives of root variables evaluated at different points\n",
        "    '''\n",
        "\n",
        "```\n",
        "\n",
        "3) A class **ReverseVecFunc** which calculate the value and derivative for vector functions evaluated at different points\n",
        "```python \n",
        "\n",
        "class ReverseVecFunc:\n",
        "\n",
        "  def __init__(self):\n",
        "    #it stores the variables and the functions when the class is initialized.\n",
        "  def value(self,Graph):\n",
        "    #it computes the Jacobian and the value for vector functions for a given single value of variable\n",
        "  def SeriesValues(self,values,dimension,Graph):\n",
        "    '''\n",
        "    INPUT \n",
        "    =====\n",
        "    value = values of the functions\n",
        "    dimension : the number of variables\n",
        "    Graph : the graph to be connected\n",
        "    RETURN\n",
        "    ======\n",
        "    The value and the jacobian (both in 2D nparrays) of the function at a series of values\n",
        "    \n",
        "\n",
        "    '''\n",
        "    #pseudocodes\n",
        "    initialize a valuelist = []\n",
        "    for each function in vection functions:\n",
        "       calculate its gradients and values using Wrapper(args)\n",
        "       append it to a valuelist \n",
        "\n",
        "    def Wrapper(args):\n",
        "      # A helper function to help calculate the values and derivative for a series values\n",
        "      '''\n",
        "      INPUT \n",
        "      =====\n",
        "      value = values of the functions\n",
        "      dimension : the number of variables\n",
        "      Graph : the graph to be connected\n",
        "      Returns the derivatives and values of a single function evaluated different values\n",
        "      '''\n",
        "\n",
        "```\n",
        "4) Additional **ElementaryReverse.py** which defines the elementary functions operations for the Node objects\n",
        "```python\n",
        "\n",
        "'''\n",
        "NOTE\n",
        "=====\n",
        "In order to differentiate between reverse mode elementary functions and forward mode ones, all the elementary function in reverse mode will have a \"r\" as the last alphabet.\n",
        "\n",
        "If user would like to use these function with a constant , please use built in functions in Numpy: such as : np.exp(2), np.sin(3) etc.\n",
        "\n",
        "RETURN\n",
        "=====\n",
        "New Node object that stores the value after related computation and its relevant leftparent node (which is itself). The node returned wont have any rightparent node.\n",
        "\n",
        "'''\n",
        "def expr(x):\n",
        "def logr(x):\n",
        "def sqrtr(x):\n",
        "  ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QMX-moZIKJ4",
        "colab_type": "text"
      },
      "source": [
        "##Future Features\n",
        "As we have seen, automatic differentiation is an efficient and accurate way to compute derivatives, so it makes sense\n",
        "to try to apply this where we can. One of the most popular methods that use the derivative is gradient descent.\n",
        "Gradient descent is an optimization problem where we try to minimize some function. We do this by taking some starting point\n",
        "and moving in the direction\n",
        "of the steepest part of the function, or the most negative gradient. To find this minimum point, it makes sense that\n",
        "want the gradient to be precise, so the point is precise, so automatic differentiation is the natural choice.\n",
        "\n",
        "Building onto that idea, another use of this could be in neural networks. The entire concept of a neural network\n",
        "is centered around optimizing its weights, and at each step and each time the node weights update, we need to calculate\n",
        "lots of partial derivatives, so once again, it makes sense to use automatic differentiation.\n",
        "Similarly, the concept of deep learning is essentially a large collection of neural networks, so the same utility\n",
        "from automatic differentiation can be gained here.\n",
        "\n",
        "Another interesting application can be found in statistics, namely, in a Markov chain Monte Carlo sampling method, called\n",
        "the Hamiltonian Monte Carlo Algorithm. Without diving into deep statistical explanations, this is an algorithm to create a\n",
        "random sample from a probability distribution that is difficult to get normally. Most Markov chain Monte Carlo algorithms\n",
        "coverge quite slowly, and as a result explore the sampling slowly as well. The hamiltonian algorithm\n",
        "does a better job with convergence, but at the cost of having to evaluate complicated gradients of probability models along the way. \n",
        "However, with automatic differentiation, the user no longer has to manually derive the gradients, and this cost can be reduced significantly.\n",
        "\n",
        "ref:http://jmlr.org/papers/volume18/17-468/17-468.pdf"
      ]
    }
  ]
}